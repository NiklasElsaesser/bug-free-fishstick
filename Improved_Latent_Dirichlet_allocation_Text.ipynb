{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiklasElsaesser/bug-free-fishstick/blob/main/deprecated_Improved_Latent_Dirichlet_allocation_V4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFSUE1lr3tRw"
      },
      "source": [
        "Version based on: [LDAPrototype](https://github.com/JonasRieger/ldaPrototype?tab=readme-ov-file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup\n",
        "Installing the necessary Libraries via pip to among other things enable the setup in various development environments. The primary (default) development of the Notebook was done in Google Colab, but also occasionally run Visual Studio Code.\n",
        "\n",
        "Notable Libraries here are:\n",
        "\n",
        "\n",
        "*   gensim ->  library for topic modelling, document indexing and similarity retrieval with large corpora\n",
        "*   scikit-learn -> module for machine learning built on top of SciPy\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg31CMXsgOat",
        "outputId": "d0cdf39f-066e-4fed-919d-06a9eb3c8be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/lib/python3.11/site-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/lib/python3.11/site-packages (1.24.4)\n",
            "Requirement already satisfied: gensim in /opt/homebrew/anaconda3/lib/python3.11/site-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from gensim) (1.24.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: matplotlib in /opt/homebrew/anaconda3/lib/python3.11/site-packages (3.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.24.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: wordcloud in /opt/homebrew/anaconda3/lib/python3.11/site-packages (1.9.3)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from wordcloud) (1.24.4)\n",
            "Requirement already satisfied: pillow in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from wordcloud) (3.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (24.1)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
            "Requirement already satisfied: seaborn in /opt/homebrew/anaconda3/lib/python3.11/site-packages (0.12.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from seaborn) (1.24.4)\n",
            "Requirement already satisfied: pandas>=0.25 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from seaborn) (3.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas>=0.25->seaborn) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /opt/homebrew/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
            "Requirement already satisfied: bottleneck in /opt/homebrew/anaconda3/lib/python3.11/site-packages (1.4.0)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from bottleneck) (1.24.4)\n",
            "Requirement already satisfied: pyldavis in /opt/homebrew/anaconda3/lib/python3.11/site-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pyldavis) (1.24.4)\n",
            "Requirement already satisfied: scipy in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pyldavis) (1.10.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pyldavis) (2.2.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pyldavis) (1.2.0)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pyldavis) (3.1.2)\n",
            "Requirement already satisfied: numexpr in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pyldavis) (2.8.4)\n",
            "Requirement already satisfied: funcy in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pyldavis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pyldavis) (1.3.0)\n",
            "Requirement already satisfied: gensim in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pyldavis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pyldavis) (68.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyldavis) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from pandas>=2.0.0->pyldavis) (2023.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from scikit-learn>=1.0.0->pyldavis) (2.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from gensim->pyldavis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from jinja2->pyldavis) (2.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyldavis) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install --upgrade gensim\n",
        "!pip install matplotlib\n",
        "!pip install wordcloud\n",
        "!pip install seaborn\n",
        "!pip install scikit-learn\n",
        "!pip install --upgrade bottleneck\n",
        "!pip install pyldavis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here the required Moduels are imported from the previously installed libraries.\n",
        "\n",
        "Notable Modules are:\n",
        "\n",
        "\n",
        "**Gensim**\n",
        "*   corpora **->** for the creation of dicitonaries and document-term matrices (corpus) for topic modeling.\n",
        "*   models.CoherenceModel **->** to assess the coherence (quality) of topics and validate the results generated by the LDA models.\n",
        "\n",
        "Rehurek, R., & Sojka, P. (2011). Gensim–python framework for vector space modelling. NLP Centre, Faculty of Informatics, Masaryk University, Brno, Czech Republic, 3(2).\n",
        "\n",
        "**Sklearn**\n",
        "* metrics.pairwise.cosine_similarity **->** computing the cosine similarity between samples in X and Y, in this case calculating vectors, document similarity and clustering text based on the content.\n",
        "* feature_extraction.text.TfidfVectorizer **->**  reshaping raw text into matrix of TF-IDF features, weighing the significance of terms in a document by their relation of occurrence in the entire corpus.\n",
        "\n",
        "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830.\n",
        "\n",
        "\n",
        "Other:\n",
        "* matplotlib, seaborn and pyldavis for visualizations\n",
        "* pandas and numpy for data manipulation and scientific computing\n",
        "* random to create a seed with random parameters when running multiple ldas\n",
        "* re to prepare the dataset and remove values based on regex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cDcgsNkTXidV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaFZQ5LJfCo6",
        "outputId": "49553dac-b55f-43ee-b3a9-f4141042bb51"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZxMPl9n9zGH"
      },
      "source": [
        "# **@Anna Datenbereinigung**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRtrhvARgPAu",
        "outputId": "0ea80cd7-1cf5-4c62-a98f-6b4eb28a63c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                link  \\\n",
            "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
            "1  https://www.huffpost.com/entry/american-airlin...   \n",
            "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
            "3  https://www.huffpost.com/entry/funniest-parent...   \n",
            "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
            "\n",
            "                                            headline   category  \\\n",
            "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
            "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
            "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
            "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
            "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
            "\n",
            "                                   short_description               authors  \\\n",
            "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
            "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
            "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
            "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
            "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
            "\n",
            "         date  \n",
            "0  2022-09-23  \n",
            "1  2022-09-23  \n",
            "2  2022-09-23  \n",
            "3  2022-09-23  \n",
            "4  2022-09-22  \n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "dataset_path = \"/Users/niklaselsasser/Code/bug-free-fishstick/News_Category_Dataset_v3.json\"\n",
        "\n",
        "with open(dataset_path, 'r') as file:\n",
        "    news_data = [json.loads(line) for line in file]\n",
        "\n",
        "# Convert to DataFrame for easy inspection\n",
        "df = pd.DataFrame(news_data)\n",
        "\n",
        "# Check the structure of the data\n",
        "print(df.head())\n",
        "\n",
        "# Example columns to expect: 'category', 'headline', 'short_description', 'link', 'authors', 'date'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWeEkEAg9-PW"
      },
      "source": [
        "# **@Anna Datenbereinigung**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "nKPsCA2NepVr",
        "outputId": "e1cf2808-0062-4574-cb59-9ef234332340"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>category</th>\n",
              "      <th>short_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>Health experts said it is too early to predict...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>He was subdued by passengers and crew when he ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
              "      <td>COMEDY</td>\n",
              "      <td>\"Until you have a dog you don't understand wha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
              "      <td>PARENTING</td>\n",
              "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline   category  \\\n",
              "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
              "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
              "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
              "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
              "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
              "\n",
              "                                   short_description  \n",
              "0  Health experts said it is too early to predict...  \n",
              "1  He was subdued by passengers and crew when he ...  \n",
              "2  \"Until you have a dog you don't understand wha...  \n",
              "3  \"Accidentally put grown-up toothpaste on my to...  \n",
              "4  Amy Cooper accused investment firm Franklin Te...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_df=df.drop(columns=['link','authors','date'])\n",
        "clean_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rj3kwmz9_cH"
      },
      "source": [
        "# **@Anna Datenbereinigung**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxwcpqmnESk7",
        "outputId": "4fd489f6-3c50-499c-a9fb-b2a86cd5574c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/niklaselsasser/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/niklaselsasser/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/niklaselsasser/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    category                                     processed_text\n",
            "0  U.S. NEWS  [4, million, american, roll, sleeve, omicronta...\n",
            "1  U.S. NEWS  [american, airline, flyer, charged, banned, li...\n",
            "2     COMEDY  [23, funniest, tweet, cat, dog, week, sept, 17...\n",
            "3  PARENTING  [funniest, tweet, parent, week, sept, 1723, ac...\n",
            "4  U.S. NEWS  [woman, called, cop, black, birdwatcher, loses...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and non-alphabetical characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and lemmatize\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to the headlines and short descriptions\n",
        "clean_df['processed_text'] = clean_df.apply(lambda row: preprocess_text(row['headline'] + ' ' + row['short_description']), axis=1)\n",
        "\n",
        "# Filter out empty processed_text\n",
        "processed_df = clean_df[clean_df['processed_text'].apply(len) > 0]\n",
        "\n",
        "# Inspect the processed text\n",
        "print(processed_df[['category', 'processed_text']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "tZ_G1bIFhIr5",
        "outputId": "fef69a67-c53f-4356-a2b7-614edccd0a86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>category</th>\n",
              "      <th>short_description</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>Health experts said it is too early to predict...</td>\n",
              "      <td>[4, million, american, roll, sleeve, omicronta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>He was subdued by passengers and crew when he ...</td>\n",
              "      <td>[american, airline, flyer, charged, banned, li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
              "      <td>COMEDY</td>\n",
              "      <td>\"Until you have a dog you don't understand wha...</td>\n",
              "      <td>[23, funniest, tweet, cat, dog, week, sept, 17...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
              "      <td>PARENTING</td>\n",
              "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
              "      <td>[funniest, tweet, parent, week, sept, 1723, ac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
              "      <td>[woman, called, cop, black, birdwatcher, loses...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline   category  \\\n",
              "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
              "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
              "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
              "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
              "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
              "\n",
              "                                   short_description  \\\n",
              "0  Health experts said it is too early to predict...   \n",
              "1  He was subdued by passengers and crew when he ...   \n",
              "2  \"Until you have a dog you don't understand wha...   \n",
              "3  \"Accidentally put grown-up toothpaste on my to...   \n",
              "4  Amy Cooper accused investment firm Franklin Te...   \n",
              "\n",
              "                                      processed_text  \n",
              "0  [4, million, american, roll, sleeve, omicronta...  \n",
              "1  [american, airline, flyer, charged, banned, li...  \n",
              "2  [23, funniest, tweet, cat, dog, week, sept, 17...  \n",
              "3  [funniest, tweet, parent, week, sept, 1723, ac...  \n",
              "4  [woman, called, cop, black, birdwatcher, loses...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0IInK5ELjtL"
      },
      "source": [
        "Step 1-4 are from LDAPrototype **Paper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzHEi_WYGSXb",
        "outputId": "2fe1e73e-0e61-4e8e-ca30-376b807ad2b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3t/mgckmfnn5p745wd8_80ywb0c0000gn/T/ipykernel_82713/2393919608.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  processed_df['processed_text'] = processed_df['processed_text'].apply(lambda x: [word for word in x if not re.search(r'\\d', word)])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            headline   category  \\\n",
            "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
            "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
            "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
            "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
            "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
            "\n",
            "                                   short_description  \\\n",
            "0  Health experts said it is too early to predict...   \n",
            "1  He was subdued by passengers and crew when he ...   \n",
            "2  \"Until you have a dog you don't understand wha...   \n",
            "3  \"Accidentally put grown-up toothpaste on my to...   \n",
            "4  Amy Cooper accused investment firm Franklin Te...   \n",
            "\n",
            "                                      processed_text  \n",
            "0  [million, american, roll, sleeve, omicrontarge...  \n",
            "1  [american, airline, flyer, charged, banned, li...  \n",
            "2  [funniest, tweet, cat, dog, week, sept, dog, d...  \n",
            "3  [funniest, tweet, parent, week, sept, accident...  \n",
            "4  [woman, called, cop, black, birdwatcher, loses...  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/3t/mgckmfnn5p745wd8_80ywb0c0000gn/T/ipykernel_82713/2393919608.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  processed_df['processed_text'] = processed_df['processed_text'].apply(lambda x: [word for word in x if len(word) > 2])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 1. Remove all numbers from the processed_text column\n",
        "processed_df['processed_text'] = processed_df['processed_text'].apply(lambda x: [word for word in x if not re.search(r'\\d', word)])\n",
        "\n",
        "# 2. Flatten processed_text for TF-IDF input\n",
        "corpus = [\" \".join(tokens) for tokens in processed_df['processed_text']]\n",
        "\n",
        "# 3. Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=10)  # min_df removes terms that appear in fewer than 10 documents\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# 4. Get feature names (words) and sort them by importance\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "#Some very short words (e.g., one or two characters like \"a\", \"is\", etc.) may still remain despite removing stopwords, which might not be very informative.\n",
        "processed_df['processed_text'] = processed_df['processed_text'].apply(lambda x: [word for word in x if len(word) > 2])\n",
        "\n",
        "\n",
        "# Show the modified DataFrame\n",
        "print(processed_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "2SPM4DI0Izqo",
        "outputId": "b7feda94-529d-45e4-864a-43556a7577bd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>category</th>\n",
              "      <th>short_description</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>Health experts said it is too early to predict...</td>\n",
              "      <td>[million, american, roll, sleeve, omicrontarge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>He was subdued by passengers and crew when he ...</td>\n",
              "      <td>[american, airline, flyer, charged, banned, li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
              "      <td>COMEDY</td>\n",
              "      <td>\"Until you have a dog you don't understand wha...</td>\n",
              "      <td>[funniest, tweet, cat, dog, week, sept, dog, d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
              "      <td>PARENTING</td>\n",
              "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
              "      <td>[funniest, tweet, parent, week, sept, accident...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
              "      <td>[woman, called, cop, black, birdwatcher, loses...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline   category  \\\n",
              "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
              "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
              "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
              "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
              "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
              "\n",
              "                                   short_description  \\\n",
              "0  Health experts said it is too early to predict...   \n",
              "1  He was subdued by passengers and crew when he ...   \n",
              "2  \"Until you have a dog you don't understand wha...   \n",
              "3  \"Accidentally put grown-up toothpaste on my to...   \n",
              "4  Amy Cooper accused investment firm Franklin Te...   \n",
              "\n",
              "                                      processed_text  \n",
              "0  [million, american, roll, sleeve, omicrontarge...  \n",
              "1  [american, airline, flyer, charged, banned, li...  \n",
              "2  [funniest, tweet, cat, dog, week, sept, dog, d...  \n",
              "3  [funniest, tweet, parent, week, sept, accident...  \n",
              "4  [woman, called, cop, black, birdwatcher, loses...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "processed_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqWb2G2F-DlH"
      },
      "source": [
        "# **@Anna Daten vorbereitung für LDA**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "41VJNNFpmztq"
      },
      "outputs": [],
      "source": [
        "# Create a Gensim dictionary from the 'preprocessed_text' column\n",
        "dictionary = corpora.Dictionary(processed_df['processed_text'])\n",
        "\n",
        "# Filter out extremes to limit the number of features\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
        "\n",
        "# Convert tokenized documents into a Bag of Words (BOW) format\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_df['processed_text']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZyOt-fNzyMJ"
      },
      "source": [
        "# **@Anna Datenbearbeitung Ende**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lSGvM8jzzu4"
      },
      "source": [
        "## LDA Ausführung - Singlecore\n",
        "1. V0 10 topics, 4 runs, 15 passes - complete\n",
        "2. V1 15 topics, 5 runs, 15 passes - complete\n",
        "3. V2 30 topics, 10 runs, 20 passes - complete\n",
        "4. V3 10 topics, 4 runs, 30 passes - complete\n",
        "5. V4 30 topics, 10 runs, 30 passes - complete\n",
        "6. V5 10 topics, 2 runs, 50 passes - complete\n",
        "7. V6 num_topics=10, num_runs=40, passes=10, corpus large - complete\n",
        "8.  VSC1 num_topics=15, num runs=2, passes=20, corpus[:1000] - complete\n",
        "9.  VSC2 num_topics=15, num_runs=40, passes=20, corpus large - complete\n",
        "10. VSC3 num_topics=50, num_runs=1000, passes=20, corpus large - "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "XoopXk-eXgdS",
        "outputId": "a42ffe46-025f-4682-e71f-fa96949bf088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run 1/1000 started at 2024-10-11 08:37:17 and ended at 2024-10-11 08:44:10\n",
            "Run 1 completed in 6.88 minutes\n",
            "Run 2/1000 started at 2024-10-11 08:44:10 and ended at 2024-10-11 08:51:05\n",
            "Run 2 completed in 6.91 minutes\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "import gensim\n",
        "\n",
        "def run_multiple_ldas(corpus, dictionary, num_topics=50, num_runs=1000, passes=20, random_state=None):\n",
        "    lda_models = []\n",
        "    for i in range(num_runs):\n",
        "        seed = random.randint(1, 10000) if random_state is None else random_state + i\n",
        "\n",
        "        start_time = time.time()  # Start the timer\n",
        "        start_clock_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(start_time))  # Record start clock time\n",
        "\n",
        "        lda = gensim.models.LdaModel(\n",
        "            corpus=corpus,\n",
        "            id2word=dictionary,\n",
        "            num_topics=num_topics,\n",
        "            random_state=seed,\n",
        "            passes=passes,\n",
        "            alpha='auto',\n",
        "            eta='auto'\n",
        "        )\n",
        "\n",
        "        end_time = time.time()  # End the timer\n",
        "        end_clock_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(end_time))  # Record end clock time\n",
        "        duration = (end_time - start_time) / 60  # Calculate time taken in minutes\n",
        "\n",
        "        print(f'Run {i+1}/{num_runs} started at {start_clock_time} and ended at {end_clock_time}')\n",
        "        print(f'Run {i+1} completed in {duration:.2f} minutes')\n",
        "\n",
        "        lda_models.append(lda)\n",
        "\n",
        "    return lda_models\n",
        "\n",
        "# Example of running 10 LDA models with 30 topics each\n",
        "lda_models = run_multiple_ldas(corpus, dictionary, num_topics=50, num_runs=1000, passes=20, random_state=42)\n",
        "\n",
        "# test if model is working on small data corpus\n",
        "#small_corpus = corpus[:1000]  # Use the first 1000 documents as a subset\n",
        "#lda_models = run_multiple_ldas(small_corpus, dictionary, num_topics=50, num_runs=1000, passes=20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tvX_5TCrnExG"
      },
      "outputs": [],
      "source": [
        "def extract_topics(lda_models, num_words=10):\n",
        "    all_topics = []\n",
        "    for lda in lda_models:\n",
        "        topics = lda.show_topics(num_topics=-1, num_words=num_words, formatted=False)\n",
        "        all_topics.append(topics)\n",
        "    return all_topics\n",
        "\n",
        "all_topics = extract_topics(lda_models, num_words=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gwvXALvFnIB7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "def get_topic_vectors(lda_models, dictionary, num_words=100):\n",
        "    # Create term-document matrix for all topics across all models\n",
        "    topic_vectors = []\n",
        "    for lda in lda_models:\n",
        "        for t in range(lda.num_topics):\n",
        "            topic = lda.get_topic_terms(t, topn=num_words)\n",
        "            vec = np.zeros(len(dictionary))\n",
        "            for term_id, weight in topic:\n",
        "                vec[term_id] = weight\n",
        "            topic_vectors.append(vec)\n",
        "    return topic_vectors\n",
        "\n",
        "def calculate_similarity_matrix(topic_vectors, similarity='jaccard'):\n",
        "    # Convert topic_vectors to a NumPy array if it is not already\n",
        "    topic_vectors = np.array(topic_vectors)\n",
        "\n",
        "    if similarity == 'cosine':\n",
        "        # Compute cosine similarity\n",
        "        return cosine_similarity(topic_vectors)\n",
        "\n",
        "    elif similarity == 'jaccard':\n",
        "        # For Jaccard, binarize the vectors\n",
        "        bin_vectors = (topic_vectors > 0).astype(int)\n",
        "        intersection = np.dot(bin_vectors, bin_vectors.T)\n",
        "        row_sums = bin_vectors.sum(axis=1)\n",
        "        union = row_sums[:, None] + row_sums - intersection\n",
        "        return intersection / union\n",
        "\n",
        "    elif similarity == 'jsd':\n",
        "        # Compute Jensen-Shannon divergence, note it returns distance, so we subtract from 1\n",
        "        n = len(topic_vectors)\n",
        "        js_matrix = np.zeros((n, n))\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                js_matrix[i, j] = jensenshannon(topic_vectors[i], topic_vectors[j])\n",
        "        return 1 - js_matrix\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown similarity measure: {similarity}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2OK9Edn5nMHN"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Assuming the function get_topic_vectors and calculate_similarity_matrix are already defined\n",
        "\n",
        "# 1. Generate topic vectors from LDA models\n",
        "topic_vectors = get_topic_vectors(lda_models, dictionary, num_words=100)\n",
        "\n",
        "# 2. Calculate cosine similarity matrix\n",
        "cosine_sim_matrix = calculate_similarity_matrix(topic_vectors, similarity='cosine')\n",
        "\n",
        "# 3. Perform clustering (Agglomerative Clustering in this example)\n",
        "# Metric = 'precomputed' means we provide a distance matrix, so we use 1 - cosine_sim_matrix as distance\n",
        "clustering_model_cosine = AgglomerativeClustering(n_clusters=5, metric='precomputed', linkage='average')\n",
        "cluster_labels_cosine = clustering_model_cosine.fit_predict(1 - cosine_sim_matrix)\n",
        "\n",
        "# 4. Define the function to select prototype topics from each cluster\n",
        "def select_prototype_topics(all_topics, cluster_labels):\n",
        "    prototypes = []\n",
        "    for cluster in np.unique(cluster_labels):\n",
        "        # Get indices of topics in the current cluster\n",
        "        cluster_indices = np.where(cluster_labels == cluster)[0]\n",
        "\n",
        "        # Select the prototype topic (you can define your own criteria, here we just pick the first topic)\n",
        "        prototype_topic = all_topics[cluster_indices[0]]  # Modify this selection logic if needed\n",
        "        prototypes.append(prototype_topic)\n",
        "\n",
        "    return prototypes\n",
        "\n",
        "# 5. Assuming `all_topics` is available (list of topics from LDA models)\n",
        "prototype_topics_cosine = select_prototype_topics(topic_vectors, cluster_labels_cosine)\n",
        "\n",
        "# 6. Now you have the prototypes for the cosine similarity clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKkcmxdenPup"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def assign_prototype_topics(lda_models, corpus, prototype_topics, dictionary):\n",
        "    # Create a single LDA model from prototypes for assignment\n",
        "    # Alternatively, compute similarity between document-topic distributions and prototypes\n",
        "    # Here, we'll assign based on the highest topic probability across all models\n",
        "    doc_topics = []\n",
        "\n",
        "    for i, bow in enumerate(corpus):\n",
        "        print(f\"\\nProcessing document {i + 1}/{len(corpus)}...\")  # Print the current document number\n",
        "        topic_probs = []\n",
        "\n",
        "        for j, lda in enumerate(lda_models):\n",
        "            # Get the topic probabilities for the current document\n",
        "            probs = lda.get_document_topics(bow, minimum_probability=0)\n",
        "            # Extract the probabilities and print them\n",
        "            prob_values = [prob for _, prob in probs]\n",
        "            topic_probs.extend(prob_values)\n",
        "\n",
        "        # Assign the topic with the highest probability\n",
        "        if topic_probs:\n",
        "            dominant_topic = np.argmax(topic_probs)\n",
        "            doc_topics.append(dominant_topic)\n",
        "        else:\n",
        "            doc_topics.append(None)\n",
        "            print(f\"  Document {i + 1}: No topics found, assigned None.\")\n",
        "\n",
        "    return doc_topics\n",
        "\n",
        "# Use .loc to set the dominant_topic column to avoid the warning\n",
        "processed_df.loc[:, 'dominant_topic'] = assign_prototype_topics(lda_models, corpus, prototype_topics_cosine, dictionary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuJ-5m2afPCb"
      },
      "outputs": [],
      "source": [
        "print(processed_df.columns)\n",
        "print(processed_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKBAKIamnSWm"
      },
      "outputs": [],
      "source": [
        "def compute_coherence(lda_model, texts, dictionary, coherence='c_v'):\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence=coherence)\n",
        "    return coherence_model.get_coherence()\n",
        "\n",
        "# Compute coherence for each LDA model\n",
        "for i, lda in enumerate(lda_models):\n",
        "    coherence = compute_coherence(lda, processed_df['processed_text'], dictionary)\n",
        "    print(f'LDA Model {i+1} Coherence: {coherence:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJezzW5Jn72T"
      },
      "source": [
        "# visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7jPokr4nZR0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot the distribution of dominant topics\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x='dominant_topic', data=processed_df)\n",
        "plt.title('Distribution of Dominant Topics')\n",
        "plt.xlabel('Topic')\n",
        "plt.ylabel('Number of Documents')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9nQr5trZnb6T"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis.gensim_models\n",
        "import pyLDAvis\n",
        "\n",
        "# Prepare visualization for the first LDA model\n",
        "lda_visualization = pyLDAvis.gensim_models.prepare(lda_models[0], corpus, dictionary)\n",
        "pyLDAvis.display(lda_visualization)\n",
        "\n",
        "# Save the visualization to an HTML file in folder B:\n",
        "#pyLDAvis.save_html(lda_visualization, '/Users/niklaselsasser/Code/bug-free-fishstick/visuals/VSC3_lda_visualization.html')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTWgp-hyfepT"
      },
      "source": [
        "# Evaluating topics against values of category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IHnt4BE0oqAv"
      },
      "outputs": [],
      "source": [
        "def get_top_words_per_topic(lda_model, num_words=10):\n",
        "    top_words_per_topic = {}\n",
        "    for topic_id in range(lda_model.num_topics):\n",
        "        top_words = lda_model.show_topic(topic_id, topn=num_words)\n",
        "        top_words_per_topic[topic_id] = [word for word, _ in top_words]\n",
        "    return top_words_per_topic\n",
        "\n",
        "# Get top words for the first LDA model as an example\n",
        "top_words = get_top_words_per_topic(lda_models[0], num_words=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(corpus))  # Should be equal to the number of rows in processed_df\n",
        "print(processed_df.shape[0])  # Number of rows in processed_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Nrq7_Z4_otZY"
      },
      "outputs": [],
      "source": [
        "# Define the function to assign dominant topics\n",
        "def assign_dominant_topic(lda_model, corpus):\n",
        "    dominant_topics = []\n",
        "    for bow in corpus:\n",
        "        topic_probs = lda_model.get_document_topics(bow, minimum_probability=0)\n",
        "        dominant_topic = sorted(topic_probs, key=lambda x: x[1], reverse=True)[0][0]\n",
        "        dominant_topics.append(dominant_topic)\n",
        "    return dominant_topics\n",
        "\n",
        "# Step to assign dominant topics\n",
        "dominant_topics = assign_dominant_topic(lda_models[0], corpus)\n",
        "\n",
        "# Use .loc to avoid SettingWithCopyWarning\n",
        "processed_df.loc[:, 'dominant_topic'] = dominant_topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTbaQoQ_ho-r"
      },
      "outputs": [],
      "source": [
        "# Checking the DataFrame after assigning dominant topics\n",
        "print(processed_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGoqr6ZCovlv"
      },
      "outputs": [],
      "source": [
        "def map_topics_to_categories(processed_df):\n",
        "    topic_category_mapping = {}\n",
        "    for topic in processed_df['dominant_topic'].unique():\n",
        "        # Get documents assigned to the topic\n",
        "        topic_docs = processed_df[processed_df['dominant_topic'] == topic]\n",
        "        # Find the most common category among these documents\n",
        "        if not topic_docs.empty:\n",
        "            most_common_category = topic_docs['category'].mode()[0]\n",
        "            topic_category_mapping[topic] = most_common_category\n",
        "    return topic_category_mapping\n",
        "\n",
        "# Create the mapping\n",
        "topic_category_mapping = map_topics_to_categories(processed_df)\n",
        "print(\"Topic to Category Mapping:\")\n",
        "print(topic_category_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PifCpRG8oxoO"
      },
      "outputs": [],
      "source": [
        "def assign_predicted_category(processed_df, topic_category_mapping):\n",
        "    processed_df['predicted_category'] = processed_df['dominant_topic'].map(topic_category_mapping)\n",
        "    return processed_df\n",
        "\n",
        "# Assign predicted categories\n",
        "processed_df = assign_predicted_category(processed_df, topic_category_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0w9NMgho0Pt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Remove any rows where 'predicted_category' is NaN\n",
        "evaluation_df = processed_df.dropna(subset=['predicted_category'])\n",
        "\n",
        "# Actual and predicted categories\n",
        "y_true = evaluation_df['category']\n",
        "y_pred = evaluation_df['predicted_category']\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "print(f\"Accuracy Score: {accuracy_score(y_true, y_pred):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJOMaQDno3R7"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.ylabel('Actual Category')\n",
        "    plt.xlabel('Predicted Category')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Get list of unique categories\n",
        "categories = sorted(processed_df['category'].unique())\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred, labels=categories)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, labels, save_path=None):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels, yticklabels=labels)\n",
        "    plt.ylabel('Actual Category')\n",
        "    plt.xlabel('Predicted Category')\n",
        "    plt.title('Confusion Matrix')\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, format='png')\n",
        "    plt.show()\n",
        "\n",
        "# Get list of unique categories\n",
        "categories = sorted(processed_df['category'].unique())\n",
        "\n",
        "# Plot and save confusion matrix\n",
        "plot_confusion_matrix(y_true, y_pred, labels=categories, save_path='/Users/niklaselsasser/Code/bug-free-fishstick/visuals/VSC3_confusion_matrix.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inqpgRJqoSYe"
      },
      "source": [
        "# further visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PqeCJCdq22F"
      },
      "outputs": [],
      "source": [
        "processed_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us6SJNjYoR8L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.table import Table\n",
        "\n",
        "# Assuming 'processed_df' already contains 'headline', 'category', and 'dominant_topic'\n",
        "\n",
        "# Add a column for the top words of the dominant topic for each document\n",
        "def get_top_words_for_topic(lda_model, topic_id, num_words=5):\n",
        "    return \", \".join([word for word, _ in lda_model.show_topic(topic_id, topn=num_words)])\n",
        "\n",
        "# Add the top words for the dominant topic to the dataframe\n",
        "processed_df['generated_topic'] = processed_df['dominant_topic'].apply(\n",
        "    lambda topic_id: get_top_words_for_topic(lda_models[0], topic_id, num_words=5)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R97J8zY1tMJJ"
      },
      "outputs": [],
      "source": [
        "# Select columns for display\n",
        "df_visual = processed_df[['headline', 'short_description', 'category', 'predicted_category']]\n",
        "\n",
        "# Display a simple DataFrame table\n",
        "df_visual.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vRZdi7aEp0D8"
      },
      "outputs": [],
      "source": [
        "df_visual.to_csv('/Users/niklaselsasser/Code/bug-free-fishstick/visuals/VSC3_headline_category_generated_topic.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
